Human Activity Recognition
========================================================

### Shenda Hong

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. 

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, we use the dataset from the reference, and my goal will be:

1. Built the model to predict the manner in which they did the exercise.

2. Using cross validation and choose the best model.

More information is available from the website here: 

http://groupware.les.inf.puc-rio.br/har

## Preliminary

First of all, Let's change the workspace to proper directory and library machine learning packages. An important thing is to set a constant seed for reproducible research.

```{r results = "hide",message=FALSE, warning=FALSE}
setwd("G:\\Learn\\Coursera\\Data Science\\08_PracticalMachineLearning\\homework\\hw1")
library(caret)
library(e1071)
library(corrgram)
library(ggplot2)
library(psych)
set.seed(123123)
```

## Data preprocessing

### Loading

loading the training and testing datasets, assigning missing values to entries that are currently 'NA' or blank.

```{r}
training <- read.table("pml-training.csv", sep = ",", head = TRUE, na.strings = c("NA", ""))
testing <- read.table("pml-testing.csv", sep = ",", head = TRUE, na.strings = c("NA", ""))
```

### Dealing with missing data

Removing variables that have too much missing value.

```{r}
training <- training[, (colSums(is.na(training)) == 0)]
testing <- testing[, (colSums(is.na(testing)) == 0)]
```

### Selecting variables

Removing the first seven variables that unrelative with analysis.

```{r}
names(training)
training <- training[,-1:-7]
testing <- testing[,-1:-7]
```

### Data slicing

We now slice the training dataset into a training dataset (70% of the observations) and a validation dataset (30% of the observations). This validation dataset will allow us to perform cross validation when choosing our model.

inTrain is a random index list for slicing.

```{r}
inTrain <- createDataPartition(training$classe, p = 0.7, list = FALSE)
validation <- training[-inTrain, ]
training <- training[inTrain, ]
```

## Reducing dimension

Now, after removing 107 variables, our training set contains 53 variables. It's easier to build a prediction model. After a brief look at intercorrelations figure, we found out that many variables have high correlation. Which means that we can using PCA for dimention reducing.

We display the scree test based on the observed eigenvalues (as straight-line segments and xâ€™s), the mean eigenvalues derived from 100 random data matrices (as dashed lines), and the eigenvalues greater than 1 criteria (as a horizontal line at y=1).

Parallel analysis suggests that the number of components =  13.

```{r}
fa.parallel(training[,-53], fa="PC", main="Scree plot with parallel analysis")
preProc <- preProcess(training[,-53], method = "pca", pcaComp = 13)
trainPC <- predict(preProc, training[,-53])
validationPC <- predict(preProc, validation[,-53])
```

## Learning a model

### Model 1: Support Vector Machine (SVM)

The first model is support vector machine, which is among the best (and many believe is indeed the best) "off-the-shelf" supervised learning algorithm. 

```{r}
start <- Sys.time()
fit1 <- svm(training$classe ~ ., data = trainPC)
pred1 <- predict(fit1, validationPC)
end <- Sys.time()

time1 <- end - start
time1

res1 <- confusionMatrix(pred1, validation$classe)
res1$table
acc1 <- res1$overall["Accuracy"]
acc1 <- as.numeric(acc1)
acc1
out_of_sample_error1 <- 1 - acc1
out_of_sample_error1
```

The estimated accuracy of svm is 88.2% and the estimated out-of-sample error based on svm applied to the cross validation dataset is 11.8%.

### Model 2: Linear Discriminant Analysis (LDA)

Our second model is Linear Discriminant Analysis, which is to find a linear combination of features which characterizes or separates two or more classes of objects or events. 

```{r}
start <- Sys.time()
fit2 <- train(training$classe ~ ., method = "lda", data = trainPC)
pred2 <- predict(fit2, validationPC)
end <- Sys.time()

time2 <- end - start
time2

res2 <- confusionMatrix(pred2, validation$classe)
res2$table
acc2 <- res2$overall["Accuracy"]
acc2 <- as.numeric(acc2)
acc2
out_of_sample_error2 <- 1 - acc2
out_of_sample_error2
```

The estimated accuracy of svm is 47.5% and the estimated out-of-sample error based on svm applied to the cross validation dataset is 52.5%.

### Model 3: Random Forests (RF)

Random forests are an ensemble learning method for classification (and regression) that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes output by individual trees.

```{r}
start <- Sys.time()
fit3 <- train(training$classe ~ ., method = "rf", data = trainPC,
              trControl = trainControl(method = "cv", number = 3))
pred3 <- predict(fit3, validationPC)
end <- Sys.time()

time3 <- end - start
time3

res3 <- confusionMatrix(pred3, validation$classe)
res3$table
acc3 <- res3$overall["Accuracy"]
acc3 <- as.numeric(acc3)
acc3
out_of_sample_error3 <- 1 - acc3
out_of_sample_error3
```

The estimated accuracy of rf is 95.6% and the estimated out-of-sample error based on svm applied to the cross validation dataset is 4.4%.

### Model Comparasion

In this section, we compare our three model based on their accurate on validation set and time consuming.

```{r}
Accurate <- c(acc1, acc2, acc3)
model <- as.factor(c("SVM", "LDA", "RF"))
qplot(model, Accurate, geom = "bar", stat="identity", main = "Accuracy Comparasion")

Time <- c(time1, time2, time3 * 60)
model <- as.factor(c("SVM", "LDA", "RF"))
qplot(model, Time, geom = "bar", stat="identity", main = "Time Comparasion")
```

The above figures tell us that:

1. Random Forests have the highest accuracy but the longest time consuming.

2. Linear Discriminant Analysis have the shortest time consuming but the lowest accuracy.

3. Support Vector Machine have pretty high accuracy with the kind of short time consuming.

## Prediction

According to the analysis above, we choose svm to be our prediction model. And adjust the pca thresh to 0.99 to improve the accuracy while spending more time for a balance.

```{r}
preProc <- preProcess(training[,-53], method = "pca", thresh = 0.99)
trainPC <- predict(preProc, training[,-53])
validationPC <- predict(preProc, validation[,-53])
start <- Sys.time()
fitBest <- svm(training$classe ~ ., data = trainPC)
pred <- predict(fitBest, validationPC)
end <- Sys.time()
time <- end - start
res <- confusionMatrix(pred, validation$classe)
res$table
acc <- res$overall["Accuracy"]
acc <- as.numeric(acc)
out_of_sample_error <- 1 - acc
testPC <- predict(preProc, testing[,-53])
myPred <- predict(fitBest, testPC)
```

The time consuming of the best model is:

```{r echo = FALSE}
time
```

The accuracy is:

```{r echo = FALSE}
acc
```

The out of sample error is:

```{r echo = FALSE}
out_of_sample_error
```

Finally, my prediction on test set is:

```{r echo = FALSE}
myPred
```

## Reference

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
